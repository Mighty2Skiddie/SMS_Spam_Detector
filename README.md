# SMS_Spam_Detector
# üì® Spam Message Detector using NLP and Machine Learning

This project demonstrates a full NLP pipeline to detect **spam messages in SMS texts** using **text preprocessing, feature extraction, and machine learning classification**. We leverage traditional natural language processing techniques with `NLTK`, and implement a **Random Forest Classifier** using `Scikit-learn` to accurately distinguish between "spam" and "ham" messages. The model is trained on the popular **SMSSpamCollection** dataset and achieves an accuracy of **98.3%**.

---

## üìÅ Dataset Overview

- **Dataset:** [SMSSpamCollection Dataset](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)
- **Format:** Each row contains a label (`ham` or `spam`) and an SMS message
- **Total Records:** 5,572 messages
- The dataset is preprocessed using `Pandas`, and then passed through a complete NLP cleaning pipeline.

---

## üîÑ NLP Pipeline and Project Workflow

### 1. **Importing Libraries**

We use the following Python libraries:
- `pandas` ‚Äì for reading and manipulating data
- `nltk` ‚Äì for all NLP preprocessing (tokenization, stopwords, stemming, lemmatization)
- `sklearn` ‚Äì for machine learning model building, vectorization, and evaluation

```python
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
```
### 2. **Loading and Formatting the Dataset**
We load the `SMSSpamCollection.txt` file, where each line contains a label and a message separated by a tab character `(\t)`.
```spam = pd.read_csv("SMSSpamCollection.txt", sep="\t", names=["label", "message"])```
The dataset is then converted into a list of tuples:
`[("Free entry into a contest", "spam"), ("Hey there, how are you?", "ham"), ...]`

### 3. **Text Cleaning and Preprocessing**
To prepare the raw text for machine learning, we create a robust preprocessing function:

üîß Steps Involved:
- Convert all text to lowercase

- Tokenize text into words

- Remove stopwords (common words like ‚Äúis‚Äù, ‚Äúthe‚Äù, ‚Äúyou‚Äù that don‚Äôt contribute to meaning)

- Apply stemming or lemmatization to reduce words to their base/root form

  -- Stemming cuts words roughly (e.g., "loving" ‚Üí "love")

  -- Lemmatization uses dictionary rules (e.g., "running" ‚Üí "run")

```
def preprocess(document, stem=True):
    document = document.lower()
    words = word_tokenize(document)
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.isalpha() and word not in stop_words]
    
    if stem:
        stemmer = PorterStemmer()
        words = [stemmer.stem(word) for word in words]
    else:
        lemmatizer = WordNetLemmatizer()
        words = [lemmatizer.lemmatize(word) for word in words]
        
    return ' '.join(words)
```
This function is applied to all messages in the dataset before feature extraction.

### 4. **Feature Extraction using Bag of Words (BoW)**
We use CountVectorizer from `sklearn` to convert preprocessed text into a numeric format that machine learning models can understand.

üîç How it works:
- Builds a vocabulary of all unique words across the dataset

- Each message becomes a vector with frequency counts of these words (sparse matrix)

```
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(cleaned_messages)  # cleaned_messages is a list of preprocessed messages
```

### 5. **Label Encoding and Splitting Data**
The target labels (`spam` and `ham)` are converted into binary form (1 and 0) and the dataset is split into training and testing sets:
```
y = spam['label'].map({'ham': 0, 'spam': 1})
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### 6. **Model Training with Random Forest Classifier**

Random Forest is used because:

- It handles high-dimensional sparse matrices well

- It is less prone to overfitting compared to individual decision trees

- Works great with small- to medium-sized datasets like ours
```
model = RandomForestClassifier()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```
### 7. **Model Evaluation**
We evaluate the model‚Äôs performance using:

- Accuracy Score

- Confusion Matrix

- Classification Report (Precision, Recall, F1-score)
```
print("Accuracy:", accuracy_score(y_test, predictions))
print("Confusion Matrix:\n", confusion_matrix(y_test, predictions))
print("Classification Report:\n", classification_report(y_test, predictions))
```
## **Results**

| Metric           | Value   |
| ---------------- | ------- |
| Accuracy         | \~98.3% |
| Precision (Spam) | \~0.98  |
| Recall (Spam)    | \~0.96  |
| F1 Score         | \~0.97  |

## **Technologies & Libraries Used**
- Python: Core programming language

- Pandas: Data reading and manipulation

- NLTK: Natural language processing (tokenization, stopwords, stemming, lemmatization)

- Scikit-learn: Model building (Random Forest), feature extraction (CountVectorizer), evaluation metrics

- Matplotlib / Seaborn (optional): For visualizing confusion matrix and feature importances

## **How to Run This Project**

### 1. **Clone the repository**
```
git clone https://github.com/yourusername/Spam-Detector-NLP.git
cd Spam-Detector-NLP
```
### 2. **Install dependencies**
```
pip install pandas nltk scikit-learn matplotlib seaborn
```
### 3. **Download required NLTK corpora**
```
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
```
### 4. **Run the notebook**
```
jupyter notebook Spam_Detector.ipynb
```

## **Future Enhancements**

- Replace BoW with TF-IDF vectorization for better semantic capture

- Try models like Naive Bayes, SVM, or LSTM for sequence learning

- Implement streamlit-based UI for real-time spam prediction

- Deploy the model using Flask/FastAPI as a RESTful web service

- Track model performance with TensorBoard or MLflow



